version: '3.8'

services:
  # Llama 3.2 (3.21B) - Lightweight general-purpose model
  llama32:
    image: ollama/ollama:latest
    container_name: llama32_model
    ports:
      - "11434:11434"
    volumes:
      - llama32_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    command: >
      bash -c "
        ollama serve &
        sleep 10 &&
        ollama pull llama3.2:3b &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 4G
        limits:
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Gemma 3 Quantized (3.88B) - Efficient for specialized tasks
  gemma3:
    image: ollama/ollama:latest
    container_name: gemma3_model
    ports:
      - "11435:11434"
    volumes:
      - gemma3_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    command: >
      bash -c "
        ollama serve &
        sleep 10 &&
        ollama pull gemma2:2b &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 4G
        limits:
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Phi-4 (14.66B) - Advanced reasoning and code generation
  phi4:
    image: ollama/ollama:latest
    container_name: phi4_model
    ports:
      - "11436:11434"
    volumes:
      - phi4_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    command: >
      bash -c "
        ollama serve &
        sleep 10 &&
        ollama pull phi3:14b &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 16G
        limits:
          memory: 20G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3

  # DeepSeek R1 Distill Llama (8.03B) - Reasoning and analysis
  deepseek:
    image: ollama/ollama:latest
    container_name: deepseek_model
    ports:
      - "11437:11434"
    volumes:
      - deepseek_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    command: >
      bash -c "
        ollama serve &
        sleep 10 &&
        ollama pull deepseek-coder:6.7b &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 8G
        limits:
          memory: 12G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3

  # GPT-OSS (Open Source GPT Alternative)
  gpt_oss:
    image: ollama/ollama:latest
    container_name: gpt_oss_model
    ports:
      - "11438:11434"
    volumes:
      - gpt_oss_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    command: >
      bash -c "
        ollama serve &
        sleep 10 &&
        ollama pull mistral:7b &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 8G
        limits:
          memory: 12G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Model Load Balancer & API Gateway
  model_gateway:
    image: nginx:alpine
    container_name: ai_model_gateway
    ports:
      - "8080:80"
    volumes:
      - ./config/nginx/ai_models.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - llama32
      - gemma3
      - phi4
      - deepseek
      - gpt_oss
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  llama32_data:
    driver: local
  gemma3_data:
    driver: local
  phi4_data:
    driver: local
  deepseek_data:
    driver: local
  gpt_oss_data:
    driver: local

networks:
  ai_models:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
