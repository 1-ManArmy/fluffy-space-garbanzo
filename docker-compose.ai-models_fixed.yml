version: '3.8'

services:
  # Llama 3.2 (3.21B) - IQ2_XXS/Q4_K_M Quantization
  llama32:
    image: ollama/ollama:latest
    container_name: llama32_model
    ports:
      - 11434:11434
    volumes:
      - llama32_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 15 &&
        ollama pull llama3.2:3b-instruct-q4_k_m &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 4G
        limits:
          memory: 6G
          cpus: '2.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # Gemma 3 QAT (3.88B) - Q4_0 Quantization
  gemma3:
    image: ollama/ollama:latest
    container_name: gemma3_qat_model
    ports:
      - 11435:11434
    volumes:
      - gemma3_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 15 &&
        ollama pull gemma2:2b-instruct-q4_0 &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 4G
        limits:
          memory: 6G
          cpus: '2.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # Phi-4 (14.66B) - IQ2_XXS/Q4_K_M Quantization
  phi4:
    image: ollama/ollama:latest
    container_name: phi4_model
    ports:
      - 11436:11434
    volumes:
      - phi4_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 20 &&
        ollama pull phi3:14b-medium-4k-instruct-q4_k_m &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 12G
        limits:
          memory: 16G
          cpus: '3.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # DeepSeek R1 Distill Llama (8.03B) - IQ2_XXS/Q4_K_M Quantization
  deepseek:
    image: ollama/ollama:latest
    container_name: deepseek_r1_model
    ports:
      - 11437:11434
    volumes:
      - deepseek_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 15 &&
        ollama pull deepseek-coder:6.7b-instruct-q4_k_m &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 6G
        limits:
          memory: 10G
          cpus: '2.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # GPT-OSS (Open Source GPT Alternative)
  gpt_oss:
    image: ollama/ollama:latest
    container_name: gpt_oss_model
    ports:
      - 11438:11434
    volumes:
      - gpt_oss_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 15 &&
        ollama pull mistral:7b-instruct-q4_k_m &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 6G
        limits:
          memory: 10G
          cpus: '2.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # SmolLM2 (361.82M) - IQ2_XXS/Q4_K_M Lightweight Model
  smollm2:
    image: ollama/ollama:latest
    container_name: smollm2_model
    ports:
      - 11439:11434
    volumes:
      - smollm2_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 15 &&
        ollama pull tinyllama:1.1b-chat-q4_k_m &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 1G
        limits:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # Mistral (7.25B) - IQ2_XXS/Q4_K_M Quantization
  mistral:
    image: ollama/ollama:latest
    container_name: mistral_model
    ports:
      - 11440:11434
    volumes:
      - mistral_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/root/.ollama/models
    command: >
      bash -c "
        ollama serve &
        sleep 15 &&
        ollama pull mistral:7b-instruct-v0.2-q4_k_m &&
        wait"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          memory: 6G
        limits:
          memory: 10G
          cpus: '2.0'
    healthcheck:
      test: [CMD, curl, -f, http://localhost:11434/api/version]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

  # Model Load Balancer & API Gateway
  model_gateway:
    image: nginx:alpine
    container_name: ai_model_gateway
    ports:
      - 8080:80
    volumes:
      - ./config/nginx/ai_models.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - llama32
      - gemma3
      - phi4
      - deepseek
      - gpt_oss
      - smollm2
      - mistral
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, -f, http://localhost/health]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - ai_models_network

volumes:
  llama32_data:
    driver: local
  gemma3_data:
    driver: local
  phi4_data:
    driver: local
  deepseek_data:
    driver: local
  gpt_oss_data:
    driver: local
  smollm2_data:
    driver: local
  mistral_data:
    driver: local

networks:
  ai_models_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
